{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.autograd as autograd\n",
    "import skimage.feature\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import imp\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw, ImageFilter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH='/media/bfortuner/bigguy/data/sealions/sample/'\n",
    "TRAIN_PATH=DATA_PATH+'train/'\n",
    "TRAIN_DOTTED_PATH=DATA_PATH+'traindotted/'\n",
    "TEST_PATH=DATA_PATH+'test/'\n",
    "RESULTS_PATH=DATA_PATH+'results/'\n",
    "WEIGHTS_PATH=DATA_PATH+'models/'\n",
    "IMAGE_PATH=DATA_PATH+'images/'\n",
    "PROJECT_NAME='sealions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_PATH+'train.csv')\n",
    "#sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of images with the correct counts\n",
    "print(train_data.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notes\n",
    "# cls -- sea lion class \n",
    "# tid -- train, train dotted, or test image id \n",
    "# _nb -- short for number\n",
    "# x, y -- don't forget image arrays organized row, col, channels\n",
    "\n",
    "cls_nb = 5\n",
    "        \n",
    "cls_names = (\n",
    "    'adult_males',\n",
    "    'subadult_males',\n",
    "    'adult_females',\n",
    "    'juveniles',\n",
    "    'pups',\n",
    "    'NOT_A_SEA_LION')\n",
    "\n",
    "cls_ids = [0,1,2,3,4,5]\n",
    "\n",
    "cls_names_to_id = {\n",
    "    \"adult_males\":0,\n",
    "    \"subadult_males\":1,\n",
    "    \"adult_females\":2,\n",
    "    \"juveniles\":3,\n",
    "    \"pups\":4,\n",
    "    \"NOT_A_SEA_LION\":5\n",
    "}\n",
    "\n",
    "# Average actual color of dot centers.\n",
    "cls_colors = (\n",
    "    (243,8,5),          # red\n",
    "    (244,8,242),        # magenta\n",
    "    (87,46,10),         # brown \n",
    "    (25,56,176),        # blue\n",
    "    (38,174,21),        # green\n",
    "    )\n",
    "\n",
    "train_nb = 947        \n",
    "test_nb = 18636\n",
    "        \n",
    "paths = {\n",
    "    # Source paths\n",
    "    'sample'     : os.path.join(DATA_PATH, 'sample_submission.csv'),\n",
    "    'counts'     : os.path.join(DATA_PATH, '', 'train.csv'),\n",
    "    'train'      : os.path.join(DATA_PATH, 'train', '{tid}.jpg'),\n",
    "    'dotted'     : os.path.join(DATA_PATH, 'traindotted', '{tid}.jpg'),\n",
    "    'test'       : os.path.join(DATA_PATH, 'test', '{tid}.jpg'),\n",
    "    # Data paths\n",
    "    'coords'     : os.path.join(DATA_PATH, 'coords.csv'),  \n",
    "}\n",
    "\n",
    "dot_radius = 3\n",
    "\n",
    "bad_train_ids = (\n",
    "    3, 7, 9, 21, 30, 34, 71, 81, 89, 97, 151, 184, 215, 234, 242, \n",
    "    268, 290, 311, 331, 344, 380, 384, 406, 421, 469, 475, 490, 499, \n",
    "    507, 530, 531, 605, 607, 614, 621, 638, 644, 687, 712, 721, 767, \n",
    "    779, 781, 794, 800, 811, 839, 840, 869, 882, 901, 903, 905, 909, \n",
    "    913, 927, 946)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_ids():\n",
    "    \"\"\"List of all valid train ids\"\"\"\n",
    "    tids = range(0, train_nb)\n",
    "    tids = list(set(tids) - set(bad_train_ids) )  # Remove bad ids\n",
    "    tids.sort()\n",
    "    return tids\n",
    "\n",
    "def trainsmall_ids():\n",
    "    return [i for i in range(41,51)]        # TrainSmall\n",
    "\n",
    "def get_trn_fpaths(sample=True):\n",
    "    fpaths,fnames = [],[]\n",
    "    if sample:\n",
    "        ids = trainsmall_ids()\n",
    "    else:\n",
    "        ids = train_ids()\n",
    "    fpaths = []\n",
    "    for id in ids:\n",
    "        fpaths.append(DATA_PATH+'train/'+str(id)+'.jpg')\n",
    "        fnames.append(str(id)+'.jpg')\n",
    "    return fpaths,fnames\n",
    "\n",
    "def test_ids():\n",
    "    return [i for i in range(0, test_nb)]\n",
    "\n",
    "def path(name, **kwargs):\n",
    "    \"\"\"Return path to various source files\"\"\"\n",
    "    path = paths[name].format(**kwargs)\n",
    "    return path\n",
    "\n",
    "def counts() :\n",
    "    \"\"\"A map from train_id to list of sea lion class counts\"\"\"\n",
    "    counts = {}\n",
    "    fn = path('counts')\n",
    "    with open(fn) as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            tid_counts = list(map(int, line.split(',')))\n",
    "            counts[tid_counts[0]] = tid_counts[1:]\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _load_image(itype, tid, border=0):\n",
    "    fn = path(itype, tid=tid)\n",
    "    img = np.asarray(Image.open(fn))\n",
    "    if border:\n",
    "        height, width, channels = img.shape\n",
    "        bimg = np.zeros( shape=(height+border*2, width+border*2, channels), dtype=np.uint8)\n",
    "        bimg[border:-border, border:-border, :] = img\n",
    "        img = bimg\n",
    "    return img\n",
    "\n",
    "def load_dotted_image(train_id, border=0):\n",
    "    return _load_image('dotted', train_id, border)\n",
    "\n",
    "def load_test_image(test_id, border=0):    \n",
    "    return _load_image('test', test_id, border)\n",
    "\n",
    "def load_train_image(train_id, border=0, mask=False):\n",
    "    \"\"\"Return image as numpy array\n",
    "\n",
    "    border -- add a black border of this width around image\n",
    "    mask -- If true mask out masked areas from corresponding dotted image\n",
    "    \"\"\"\n",
    "    img = _load_image('train', train_id, border)\n",
    "    if mask:\n",
    "        # The masked areas are not uniformly black, presumable due to \n",
    "        # jpeg compression artifacts\n",
    "        dot_img = _load_image('dotted', train_id, border).astype(np.uint16).sum(axis=-1)\n",
    "        img = np.copy(img)\n",
    "        img[dot_img<40] = 0\n",
    "    return img\n",
    "\n",
    "def get_paths_to_files(dir_path):\n",
    "    filepaths = []\n",
    "    fnames = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(dir_path):\n",
    "        filepaths.extend(os.path.join(dirpath, f) for f in filenames)\n",
    "        fnames.extend([f for f in filenames])\n",
    "    return filepaths, fnames\n",
    "\n",
    "def get_random_image_path(dir_path):\n",
    "    filepaths = get_paths_to_files(dir_path)[0]\n",
    "    return filepaths[random.randrange(len(filepaths))]\n",
    "\n",
    "def plot_img_array(img_arr, shape=(16,16)):\n",
    "    # Plot numpy array image\n",
    "    plt.figure(figsize=shape)\n",
    "    plt.imshow(img_arr.astype('uint8'))\n",
    "    plt.show()\n",
    "    \n",
    "def get_random_train_id(sample=True):\n",
    "    if sample:\n",
    "        trn_ids = trainsmall_ids()\n",
    "    else:\n",
    "        trn_ids = train_ids()\n",
    "    rand_id = trn_ids[random.randrange(len(trn_ids))]\n",
    "    return rand_id\n",
    "\n",
    "def get_random_train_img_arr(sample=True):\n",
    "    trn_id = get_random_train_id(sample)\n",
    "    return load_train_image(trn_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Example Images\n",
    "trn_img = load_train_image(41)\n",
    "plot_img_array(trn_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_id = get_random_train_id(True)\n",
    "trn_arr = load_train_image(trn_id)\n",
    "plot_img_array(trn_arr, shape=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_arr = get_random_train_img_arr(True)\n",
    "plot_img_array(trn_arr, shape=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dotted Image\n",
    "trn_id = get_random_train_id(True)\n",
    "dot_arr = load_dotted_image(trn_id)\n",
    "plot_img_array(dot_arr, shape=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dotted Image\n",
    "trn_id = get_random_train_id(True)\n",
    "trn_w_mask_arr = load_train_image(trn_id, mask=True)\n",
    "plot_img_array(trn_w_mask_arr, shape=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(tid_counts, true_counts) :\n",
    "\n",
    "    error = np.zeros(shape=[5] )\n",
    "\n",
    "    for tid in tid_counts:\n",
    "        true_counts = counts[tid]\n",
    "        obs_counts = tid_counts[tid]\n",
    "        diff = np.asarray(true_counts) - np.asarray(obs_counts)\n",
    "        error += diff*diff\n",
    "    print(error)\n",
    "    error /= len(tid_counts)\n",
    "    rmse = np.sqrt(error).sum() / 5\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_fpaths,trn_fnames = get_trn_fpaths(TRAIN_PATH)\n",
    "coordinates_df = pd.DataFrame(index=trn_fnames, columns=cls_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename in trn_fnames:\n",
    "    print(TRAIN_DOTTED_PATH + filename)\n",
    "    print(TRAIN_PATH + filename)\n",
    "    # read the Train and Train Dotted images\n",
    "    image_1 = cv2.imread(TRAIN_DOTTED_PATH + filename)\n",
    "    image_2 = cv2.imread(TRAIN_PATH + filename)\n",
    "\n",
    "    \n",
    "    # absolute difference between Train and Train Dotted\n",
    "    image_3 = cv2.absdiff(image_1,image_2)\n",
    "    \n",
    "    # mask out blackened regions from Train Dotted\n",
    "    mask_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)\n",
    "    mask_1[mask_1 < 20] = 0\n",
    "    mask_1[mask_1 > 0] = 255\n",
    "    \n",
    "    mask_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n",
    "    mask_2[mask_2 < 20] = 0\n",
    "    mask_2[mask_2 > 0] = 255\n",
    "    \n",
    "    image_3 = cv2.bitwise_or(image_3, image_3, mask=mask_1)\n",
    "    image_3 = cv2.bitwise_or(image_3, image_3, mask=mask_2) \n",
    "    \n",
    "    # convert to grayscale to be accepted by skimage.feature.blob_log\n",
    "    image_3 = cv2.cvtColor(image_3, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # detect blobs\n",
    "    blobs = skimage.feature.blob_log(image_3, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)\n",
    "    \n",
    "    adult_males = []\n",
    "    subadult_males = []\n",
    "    pups = []\n",
    "    juveniles = []\n",
    "    adult_females = [] \n",
    "    not_sea_lions = [] \n",
    "    \n",
    "    for blob in blobs:\n",
    "        # get the coordinates for each blob\n",
    "        y, x, s = blob\n",
    "        # get the color of the pixel from Train Dotted in the center of the blob\n",
    "        g,b,r = image_1[int(y)][int(x)][:]\n",
    "        \n",
    "        # decision tree to pick the class of the blob by looking at the color in Train Dotted\n",
    "        if r > 200 and g < 50 and b < 50: # RED\n",
    "            adult_males.append((int(x),int(y)))        \n",
    "        elif r > 200 and g > 200 and b < 50: # MAGENTA\n",
    "            subadult_males.append((int(x),int(y)))         \n",
    "        elif r < 100 and g < 100 and 150 < b < 200: # GREEN\n",
    "            pups.append((int(x),int(y)))\n",
    "        elif r < 100 and  100 < g and b < 100: # BLUE\n",
    "            juveniles.append((int(x),int(y))) \n",
    "        elif r < 150 and g < 50 and b < 100:  # BROWN\n",
    "            adult_females.append((int(x),int(y)))\n",
    "        else:  # BROWN\n",
    "            not_sea_lions.append((int(x),int(y)))\n",
    "            \n",
    "    coordinates_df[\"adult_males\"][filename] = adult_males\n",
    "    coordinates_df[\"subadult_males\"][filename] = subadult_males\n",
    "    coordinates_df[\"adult_females\"][filename] = adult_females\n",
    "    coordinates_df[\"juveniles\"][filename] = juveniles\n",
    "    coordinates_df[\"pups\"][filename] = pups\n",
    "    coordinates_df[\"NOT_A_SEA_LION\"][filename] = not_sea_lions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract 32x32 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for filename in trn_fnames:    \n",
    "    image = cv2.imread(TRAIN_PATH + filename)\n",
    "    for lion_class in cls_names:\n",
    "        for coordinates in coordinates_df[lion_class][filename]:\n",
    "            thumb = image[coordinates[1]-16:coordinates[1]+16,coordinates[0]-16:coordinates[0]+16,:]\n",
    "            if np.shape(thumb) == (32, 32, 3):\n",
    "                x.append(thumb)\n",
    "                y.append(lion_class)\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for lion_class in cls_names:\n",
    "    f, ax = plt.subplots(1,10,figsize=(12,1.5))\n",
    "    f.suptitle(lion_class)\n",
    "    axes = ax.flatten()\n",
    "    j = 0\n",
    "    for a in axes:\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "        for i in range(j,len(x)):\n",
    "            if y[i] == lion_class:\n",
    "                j = i+1\n",
    "                a.imshow(cv2.cvtColor(x[i], cv2.COLOR_BGR2RGB))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Class Names\n",
    "\n",
    "We convert the text class names into numerical ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.array([cls_names_to_id[n] for n in y])\n",
    "y[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "\n",
    "def load_img_as_np_arr(img_path):\n",
    "    return scipy.misc.imread(img_path) #scipy\n",
    "\n",
    "def get_mean_std_dataset(dir_path, sample_size=5):\n",
    "    fpaths, fnames = get_paths_to_files(dir_path)\n",
    "    total_mean = np.array([0.,0.,0.])\n",
    "    total_std = np.array([0.,0.,0.]) \n",
    "    for f in fpaths[:sample_size]:\n",
    "        img_arr = load_img_as_np_arr(f)\n",
    "        mean = np.mean(img_arr, axis=(0,1))\n",
    "        std = np.std(img_arr, axis=(0,1))\n",
    "        total_mean += mean\n",
    "        total_std += std\n",
    "    avg_mean = total_mean / sample_size\n",
    "    avg_std = total_std / sample_size\n",
    "    print(\"mean: {}\".format(avg_mean))\n",
    "    print(\"stdev: {}\".format(avg_std))\n",
    "    return avg_mean, avg_std\n",
    "\n",
    "def norm_meanstd(t, mean, std):\n",
    "    \"\"\"The Pytorch transforms module does this\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor\n",
    "    channel = (channel - mean) / std\"\"\"\n",
    "    return (t-mean) / std\n",
    "\n",
    "def denorm_meanstd(t, mean, std):\n",
    "    return (t * std) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMG_MEAN, IMG_STD = get_mean_std_dataset(TRAIN_PATH,5)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMG_MEAN, std=IMG_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "norm_x = norm_meanstd(x,IMG_MEAN,IMG_STD)\n",
    "norm_x = np.moveaxis(norm_x, 3, 1)\n",
    "print (norm_x.shape)\n",
    "print(x.mean((0,1,2)))\n",
    "print(norm_x.mean((0,1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_trn_tst_split(x,y):\n",
    "    train_size = 1000\n",
    "    idx = [i for i in range(len(x))]\n",
    "    random.shuffle(idx)\n",
    "    trn_idx = idx[:train_size]\n",
    "    tst_idx = idx[train_size:]\n",
    "    train_x = np.array([x[i] for i in trn_idx])\n",
    "    train_y = np.array([y[i] for i in trn_idx])\n",
    "    test_x = np.array([x[i] for i in tst_idx])\n",
    "    test_y = np.array([y[i] for i in tst_idx])\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "print(norm_x.shape)\n",
    "train_x, train_y, test_x, test_y = get_trn_tst_split(norm_x,y)\n",
    "print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_dset = torch.utils.data.TensorDataset(torch.from_numpy(train_x).float(),torch.from_numpy(train_y).long())\n",
    "test_dset = torch.utils.data.TensorDataset(torch.from_numpy(test_x).float(),torch.from_numpy(test_y).long())\n",
    "print(trn_dset.data_tensor.size())\n",
    "print(trn_dset.target_tensor.size())\n",
    "print(test_dset.data_tensor.size())\n",
    "print(test_dset.target_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "trn_dset_loader = torch.utils.data.DataLoader(trn_dset, batch_size=batch_size, shuffle=True)\n",
    "test_dset_loader = torch.utils.data.DataLoader(trn_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Init\n",
    "\n",
    "* http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, \n",
    "                   kernel_size=5, padding=5//2) #padding='same'?\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1  = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, \n",
    "                   kernel_size=5, padding=5//2) #padding='same'?\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.fc1   = nn.Linear(in_features=64*16*16, out_features=120) \n",
    "\n",
    "        self.fc2   = nn.Linear(in_features=120, out_features=512)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.drop2 = nn.Dropout(0.5)  \n",
    "\n",
    "        self.fc3   = nn.Linear(in_features=512, out_features=6)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "\n",
    "        x = self.fc1(x)        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss/Optimizer\n",
    "\n",
    "* http://pytorch.org/docs/optim.html?highlight=optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SimpleModel().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, trainF, epoch, projectName):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = Variable(inputs.cuda()), Variable(targets.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = get_predictions(output)\n",
    "        train_err = error(pred, targets.data.cpu())\n",
    "        partialEpoch = epoch + batch_idx / len(train_loader) - 1\n",
    "        trainF.write('{},{},{}\\n'.format(partialEpoch, loss.data[0], train_err))\n",
    "        trainF.flush()\n",
    "    print('Epoch {:d}: Train - Loss: {:.4f}\\tErr: {:.4f}'.format(epoch, loss.data[0], train_err))\n",
    "    return loss.data[0], train_err\n",
    "\n",
    "def test(model, test_loader, criterion, testF=None, epoch=1):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data.cuda(), volatile=True), Variable(target.cuda())\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "    test_loss /= len(test_loader) #n_batches\n",
    "    print('Test - Loss: {:.4f}'.format(test_loss))\n",
    "    if testF:\n",
    "        testF.write('{},{},{}\\n'.format(int(epoch), test_loss))\n",
    "        testF.flush()\n",
    "    return test_loss\n",
    "\n",
    "def predict(model, img_tensor):\n",
    "    model.eval()\n",
    "    data = Variable(img_tensor.cuda(), volatile=True)\n",
    "    output = model(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "trn_loss_history = []\n",
    "tst_loss_history = []\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    since = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trn_dset_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 500 == 499:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 500))\n",
    "            trn_loss_history.append(running_loss / 500)\n",
    "            running_loss = 0.0\n",
    "\n",
    "            \n",
    "    tst_loss = test(model,test_dset_loader,criterion,epoch=epoch)\n",
    "    tst_loss_history.append(tst_loss)\n",
    "    \n",
    "    time_elapsed = time.time() - since \n",
    "    print('Time {:.0f}m {:.0f}s\\n'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    \n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(tst_loss_history) #you can repeat this line with train loss to see both lines\n",
    "plt.title('Testset loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(trn_loss_history) #you can repeat this line with train loss to see both lines\n",
    "plt.title('Train loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_dset = datasets.ImageFolder(root=TRAIN_PATH, \n",
    "           transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "#             transforms.Normalize(torch.FloatTensor(IMG_MEAN), \n",
    "#                  torch.FloatTensor(IMG_STD)),\n",
    "        ]))\n",
    "predict_loader = torch.utils.data.DataLoader(\n",
    "    predict_dset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image, target = next(iter(predict_loader))\n",
    "image = image[0]\n",
    "print(image.size())\n",
    "\n",
    "def plot_tensor(tns_img, fs=(3,3)):\n",
    "    \"Takes a normalized tensor [0,1] and plots PIL image\"\n",
    "    pil_from_tns = transforms.ToPILImage()(tns_img)\n",
    "    plt.figure(figsize=fs)\n",
    "    plt.imshow(pil_from_tns)\n",
    "    plt.show()\n",
    "\n",
    "plot_tensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def denorm255_np_arr(arr):\n",
    "    return (arr * 255.).astype('uint8')\n",
    "\n",
    "image = denorm255_np_arr(image.numpy())\n",
    "print(image.shape)\n",
    "image = np.moveaxis(image, 0, 2)\n",
    "print(image.shape)\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_img_array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test = []\n",
    "\n",
    "for i in range(0,np.shape(image)[0],32):\n",
    "    for j in range(0,np.shape(image)[1],32):                \n",
    "        thumb = img[i:i+32,j:j+32,:]\n",
    "        if np.shape(thumb) == (32,32,3):\n",
    "            x_test.append(thumb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_img_array(x_test[0],(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slices_dset = torch.from_numpy(np.moveaxis(x_test, 3, 1)).float()\n",
    "print(slices_dset.size())\n",
    "print(slices_dset[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = predict(model, slices_dset)\n",
    "print(preds.size())\n",
    "preds.cpu().max(0)\n",
    "print (preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values, indices = preds.cpu().max(1)\n",
    "print(values.data.size(),indices.data.size())\n",
    "print(indices[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(indices.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = np.unique(indices.data.numpy(), return_counts=True)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blob Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataframe to store results in\n",
    "fpaths,fnames = get_trn_fpaths()\n",
    "# select a subset of files to run on\n",
    "fnames = fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame(index=fnames, columns=cls_names).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data[(train_data.train_id > 40) & (train_data.train_id < 51)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_1 = cv2.imread(os.path.join(TRAIN_PATH,fnames[3]))\n",
    "image_2 = cv2.imread(os.path.join(TRAIN_DOTTED_PATH, fnames[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_img_array(image_1, shape=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_2 = cv2.imread(os.path.join(TRAIN_PATH, fnames[0]))\n",
    "\n",
    "mask_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n",
    "mask_2[mask_2 < 20] = 0\n",
    "mask_2[mask_2 > 0] = 255\n",
    "\n",
    "image_5 = cv2.bitwise_or(image_2, image_2, mask=mask_2) \n",
    "\n",
    "# convert to grayscale to be accepted by skimage.feature.blob_log\n",
    "image_6 = cv2.cvtColor(image_5, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect blobs\n",
    "blobs = skimage.feature.blob_log(image_6, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in fnames:\n",
    "    image_2 = cv2.imread(os.path.join(TRAIN_PATH, filename))\n",
    "    \n",
    "    mask_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n",
    "    mask_2[mask_2 < 20] = 0\n",
    "    mask_2[mask_2 > 0] = 255\n",
    "    \n",
    "    image_5 = cv2.bitwise_or(image_2, image_2, mask=mask_2) \n",
    "    \n",
    "    # convert to grayscale to be accepted by skimage.feature.blob_log\n",
    "    image_6 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # detect blobs\n",
    "    blobs = skimage.feature.blob_log(image_6, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)\n",
    "    print\n",
    "    # prepare the image to plot the results on\n",
    "    image_7 = cv2.cvtColor(image_6, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "    for blob in blobs:\n",
    "        # get the coordinates for each blob\n",
    "        y, x, s = blob\n",
    "        cv2.circle(image_7, (int(x),int(y)), 8, (0,0,255), 2)            \n",
    "\n",
    "    # output the results\n",
    "          \n",
    "    f, ax = plt.subplots(3,2,figsize=(10,16))\n",
    "    (ax1, ax2, ax3, ax4, ax5, ax6) = ax.flatten()\n",
    "    plt.title('%s'%filename)\n",
    "    \n",
    "    ax1.imshow(cv2.cvtColor(image_2[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\n",
    "    ax1.set_title('Train')\n",
    "    ax4.imshow(cv2.cvtColor(image_5[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\n",
    "    ax4.set_title('Mask blackened areas of Train Dotted')\n",
    "    ax5.imshow(image_6[700:1200,2130:2639], cmap='gray')\n",
    "    ax5.set_title('Grayscale for input to blob_log')\n",
    "    ax6.imshow(cv2.cvtColor(image_7[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\n",
    "    ax6.set_title('Result')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename in fnames:\n",
    "    # read the Train and Train Dotted images\n",
    "    image_1 = cv2.imread(os.path.join(TRAIN_DOTTED_PATH,filename))\n",
    "    image_2 = cv2.imread(os.path.join(TRAIN_PATH, filename))\n",
    "    \n",
    "    # absolute difference between Train and Train Dotted\n",
    "    image_3 = cv2.absdiff(image_1,image_2)\n",
    "    \n",
    "    # mask out blackened regions from Train Dotted\n",
    "    mask_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)\n",
    "    mask_1[mask_1 < 20] = 0\n",
    "    mask_1[mask_1 > 0] = 255\n",
    "    \n",
    "    mask_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n",
    "    mask_2[mask_2 < 20] = 0\n",
    "    mask_2[mask_2 > 0] = 255\n",
    "    \n",
    "    image_4 = cv2.bitwise_or(image_3, image_3, mask=mask_1)\n",
    "    image_5 = cv2.bitwise_or(image_4, image_4, mask=mask_2) \n",
    "    \n",
    "    # convert to grayscale to be accepted by skimage.feature.blob_log\n",
    "    image_6 = cv2.cvtColor(image_5, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # detect blobs\n",
    "    blobs = skimage.feature.blob_log(image_6, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)\n",
    "    \n",
    "    # prepare the image to plot the results on\n",
    "    image_7 = cv2.cvtColor(image_6, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    for blob in blobs:\n",
    "        # get the coordinates for each blob\n",
    "        y, x, s = blob\n",
    "        # get the color of the pixel from Train Dotted in the center of the blob\n",
    "        b,g,r = image_1[int(y)][int(x)][:]\n",
    "        \n",
    "        # decision tree to pick the class of the blob by looking at the color in Train Dotted\n",
    "        if r > 200 and b < 50 and g < 50: # RED\n",
    "            count_df[\"adult_males\"][filename] += 1\n",
    "            cv2.circle(image_7, (int(x),int(y)), 8, (0,0,255), 2)            \n",
    "        elif r > 200 and b > 200 and g < 50: # MAGENTA\n",
    "            count_df[\"subadult_males\"][filename] += 1\n",
    "            cv2.circle(image_7, (int(x),int(y)), 8, (250,10,250), 2)            \n",
    "        elif r < 100 and b < 100 and 150 < g < 200: # GREEN\n",
    "            count_df[\"pups\"][filename] += 1\n",
    "            cv2.circle(image_7, (int(x),int(y)), 8, (20,180,35), 2) \n",
    "        elif r < 100 and  100 < b and g < 100: # BLUE\n",
    "            count_df[\"juveniles\"][filename] += 1 \n",
    "            cv2.circle(image_7, (int(x),int(y)), 8, (180,60,30), 2)\n",
    "        elif r < 150 and b < 50 and g < 100:  # BROWN\n",
    "            count_df[\"adult_females\"][filename] += 1\n",
    "            cv2.circle(image_7, (int(x),int(y)), 8, (0,42,84), 2)            \n",
    "        else:\n",
    "            count_df[\"NOT_A_SEA_LION\"][filename] += 1\n",
    "            cv2.circle(image_7, (int(x),int(y)), 8, (255,255,155), 2)\n",
    "    \n",
    "    # output the results\n",
    "          \n",
    "    f, ax = plt.subplots(3,2,figsize=(10,16))\n",
    "    (ax1, ax2, ax3, ax4, ax5, ax6) = ax.flatten()\n",
    "    plt.title('%s'%filename)\n",
    "    \n",
    "    ax1.imshow(cv2.cvtColor(image_2[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\n",
    "    ax1.set_title('Train')\n",
    "    ax2.imshow(cv2.cvtColor(image_1[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\n",
    "    ax2.set_title('Train Dotted')\n",
    "    ax3.imshow(cv2.cvtColor(image_3[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\n",
    "    ax3.set_title('Train Dotted - Train')\n",
    "    ax4.imshow(cv2.cvtColor(image_5[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\n",
    "    ax4.set_title('Mask blackened areas of Train Dotted')\n",
    "    ax5.imshow(image_6[700:1200,2130:2639], cmap='gray')\n",
    "    ax5.set_title('Grayscale for input to blob_log')\n",
    "    ax6.imshow(cv2.cvtColor(image_7[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\n",
    "    ax6.set_title('Result')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://www.kaggle.com/philschmidt/noaa-fisheries-steller-sea-lion-population-count/sea-lion-correlations-cv2-template-matching\n",
    "* https://www.kaggle.com/radustoicescu/noaa-fisheries-steller-sea-lion-population-count/get-coordinates-using-blob-detection\n",
    "* https://www.kaggle.com/asymptote/noaa-fisheries-steller-sea-lion-population-count/count-extract-sea-lions\n",
    "* https://www.kaggle.com/radustoicescu/noaa-fisheries-steller-sea-lion-population-count/use-keras-to-classify-sea-lions-0-91-accuracy\n",
    "\n",
    "**Satallite Unet Examples**\n",
    "* https://www.kaggle.com/drn01z3/dstl-satellite-imagery-feature-detection/end-to-end-baseline-with-u-net-keras/code (similar view from above on small objects)\n",
    "* https://www.kaggle.com/ceperaang/dstl-satellite-imagery-feature-detection/lb-0-42-ultimate-full-solution-run-on-your-hw/code\n",
    "* https://www.kaggle.com/amanbh/dstl-satellite-imagery-feature-detection/visualize-polygons-and-image-data/code\n",
    "\n",
    "**Spacenet challenge**\n",
    "* https://github.com/SpaceNetChallenge/BuildingDetectors\n",
    "\n",
    "**Blog Detection**\n",
    "* http://scikit-image.org/docs/dev/api/skimage.feature.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "210px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
